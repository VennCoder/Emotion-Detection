{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b47ebdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Summary:\n",
      "emotion\n",
      "calm        192\n",
      "happy       192\n",
      "sad         192\n",
      "angry       192\n",
      "fear        192\n",
      "disgust     192\n",
      "surprise    192\n",
      "neutral      96\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aksha\\anaconda3\\Lib\\site-packages\\librosa\\core\\spectrum.py:266: UserWarning: n_fft=1024 is too large for input signal of length=1012\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import librosa\n",
    "import joblib\n",
    "from librosa.feature import spectral_contrast, tonnetz\n",
    "\n",
    "class AdvancedVoiceEmotionDetector:\n",
    "    def __init__(self, dataset_path):\n",
    "        \"\"\"\n",
    "        Initialize voice emotion detection system.\n",
    "        \n",
    "        Args:\n",
    "            dataset_path (str): Path to RAVDESS dataset\n",
    "        \"\"\"\n",
    "        self.dataset_path = dataset_path\n",
    "        self.emotion_map = {\n",
    "            1: 'neutral', 2: 'calm', 3: 'happy', \n",
    "            4: 'sad', 5: 'angry', 6: 'fear', \n",
    "            7: 'disgust', 8: 'surprise'\n",
    "        }\n",
    "    \n",
    "    def create_metadata_dataframe(self):\n",
    "        \"\"\"\n",
    "        Create metadata DataFrame from RAVDESS dataset.\n",
    "        \n",
    "        Returns:\n",
    "            pandas.DataFrame: Metadata for all audio files\n",
    "        \"\"\"\n",
    "        emotion = []\n",
    "        gender = []\n",
    "        actor = []\n",
    "        file_path = []\n",
    "        \n",
    "        actor_folders = [f for f in os.listdir(self.dataset_path) if os.path.isdir(os.path.join(self.dataset_path, f))]\n",
    "        \n",
    "        for actor_folder in actor_folders:\n",
    "            actor_path = os.path.join(self.dataset_path, actor_folder)\n",
    "            filenames = os.listdir(actor_path)\n",
    "            \n",
    "            for filename in filenames:\n",
    "                parts = filename.split('.')[0].split('-')\n",
    "                emotion_code = int(parts[2])\n",
    "                emotion.append(emotion_code)\n",
    "                \n",
    "                actor_number = int(parts[6])\n",
    "                actor.append(actor_number)\n",
    "                gender.append('female' if actor_number % 2 == 0 else 'male')\n",
    "                \n",
    "                full_path = os.path.join(actor_path, filename)\n",
    "                file_path.append(full_path)\n",
    "        \n",
    "        audio_df = pd.DataFrame({\n",
    "            'emotion_code': emotion,\n",
    "            'emotion': [self.emotion_map[code] for code in emotion],\n",
    "            'gender': gender,\n",
    "            'actor': actor,\n",
    "            'path': file_path\n",
    "        })\n",
    "        \n",
    "        return audio_df\n",
    "    \n",
    "    def extract_audio_features(self, file_path, max_pad_length=100):\n",
    "        \"\"\"\n",
    "        Extract advanced audio features.\n",
    "        \n",
    "        Args:\n",
    "            file_path (str): Path to audio file\n",
    "            max_pad_length (int): Max length for feature padding\n",
    "        \n",
    "        Returns:\n",
    "            numpy.ndarray: Processed audio features or None\n",
    "        \"\"\"\n",
    "        try:\n",
    "            audio, sample_rate = librosa.load(file_path, duration=5.0)\n",
    "\n",
    "            # Extract Features\n",
    "            mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)\n",
    "            chroma = librosa.feature.chroma_stft(y=audio, sr=sample_rate)\n",
    "            mel = librosa.feature.melspectrogram(y=audio, sr=sample_rate)\n",
    "            zcr = librosa.feature.zero_crossing_rate(y=audio)\n",
    "            spectral_contrast_features = spectral_contrast(y=audio, sr=sample_rate)\n",
    "            tonnetz_features = tonnetz(y=audio, sr=sample_rate)\n",
    "\n",
    "            # Combine features\n",
    "            features = np.concatenate([\n",
    "                np.mean(mfccs, axis=1),\n",
    "                np.mean(chroma, axis=1),\n",
    "                np.mean(mel, axis=1),\n",
    "                [np.mean(zcr)],\n",
    "                np.mean(spectral_contrast_features, axis=1),\n",
    "                np.mean(tonnetz_features, axis=1)\n",
    "            ])\n",
    "\n",
    "            # Pad/truncate to fixed size\n",
    "            if len(features) > max_pad_length:\n",
    "                features = features[:max_pad_length]\n",
    "            else:\n",
    "                features = np.pad(features, (0, max_pad_length - len(features)), 'constant')\n",
    "\n",
    "            return features\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "    \n",
    "    def prepare_dataset(self, audio_df):\n",
    "        \"\"\"\n",
    "        Prepare dataset for machine learning.\n",
    "        \n",
    "        Args:\n",
    "            audio_df (pandas.DataFrame): Metadata DataFrame\n",
    "        \n",
    "        Returns:\n",
    "            tuple: Features, labels, and label encoder\n",
    "        \"\"\"\n",
    "        features = []\n",
    "        labels = []\n",
    "        \n",
    "        for _, row in audio_df.iterrows():\n",
    "            feature = self.extract_audio_features(row['path'])\n",
    "            if feature is not None:\n",
    "                features.append(feature)\n",
    "                labels.append(row['emotion'])\n",
    "        \n",
    "        X = np.array(features)\n",
    "        y = np.array(labels)\n",
    "        \n",
    "        label_encoder = LabelEncoder()\n",
    "        y_encoded = label_encoder.fit_transform(y)\n",
    "        \n",
    "        return X, y_encoded, label_encoder\n",
    "    \n",
    "    def create_model(self, input_shape, num_classes):\n",
    "        \"\"\"\n",
    "        Create a CNN-based model for voice emotion detection.\n",
    "        \n",
    "        Args:\n",
    "            input_shape (tuple): Shape of input features\n",
    "            num_classes (int): Number of emotion classes\n",
    "        \n",
    "        Returns:\n",
    "            tensorflow.keras.Model: Compiled CNN model\n",
    "        \"\"\"\n",
    "        model = Sequential([\n",
    "            Conv1D(64, kernel_size=3, activation='relu', input_shape=(input_shape[0], 1)),\n",
    "            MaxPooling1D(pool_size=2),\n",
    "\n",
    "            Conv1D(128, kernel_size=3, activation='relu'),\n",
    "            MaxPooling1D(pool_size=2),\n",
    "\n",
    "            Flatten(),\n",
    "            Dense(128, activation='relu'),\n",
    "            Dropout(0.3),\n",
    "            \n",
    "            Dense(num_classes, activation='softmax')\n",
    "        ])\n",
    "\n",
    "        model.compile(optimizer=Adam(learning_rate=0.001),\n",
    "                      loss='sparse_categorical_crossentropy',\n",
    "                      metrics=['accuracy'])\n",
    "\n",
    "        return model\n",
    "    \n",
    "    def train_and_evaluate(self, X, y, label_encoder, test_size=0.2):\n",
    "        \"\"\"\n",
    "        Train and evaluate the CNN-based emotion detection model.\n",
    "        \n",
    "        Args:\n",
    "            X (numpy.ndarray): Input features\n",
    "            y (numpy.ndarray): Label data\n",
    "            label_encoder (sklearn.preprocessing.LabelEncoder): Label encoder\n",
    "            test_size (float): Test dataset proportion\n",
    "        \n",
    "        Returns:\n",
    "            tensorflow.keras.Model: Trained model\n",
    "        \"\"\"\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=test_size, stratify=y, random_state=42\n",
    "        )\n",
    "\n",
    "        # Reshape for CNN\n",
    "        X_train = np.expand_dims(X_train, axis=-1)\n",
    "        X_test = np.expand_dims(X_test, axis=-1)\n",
    "\n",
    "        model = self.create_model(input_shape=(X_train.shape[1], 1), num_classes=len(np.unique(y)))\n",
    "\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "        model_checkpoint = ModelCheckpoint('best_voice_emotion_model.h5', save_best_only=True)\n",
    "\n",
    "        # Train model with increased epochs and batch size\n",
    "        history = model.fit(\n",
    "            X_train, y_train,\n",
    "            validation_data=(X_test, y_test),\n",
    "            epochs=200,  # Increased from 100 to 200\n",
    "            batch_size=64,  # Increased from 32 to 64\n",
    "            callbacks=[early_stopping, model_checkpoint],\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        self.evaluate_model(model, X_test, y_test, label_encoder)\n",
    "\n",
    "        return model\n",
    "    \n",
    "    def evaluate_model(self, model, X_test, y_test, label_encoder):\n",
    "        \"\"\"\n",
    "        Evaluate model performance.\n",
    "        \n",
    "        Args:\n",
    "            model (tensorflow.keras.Model): Trained model\n",
    "            X_test (numpy.ndarray): Test features\n",
    "            y_test (numpy.ndarray): Test labels\n",
    "            label_encoder (sklearn.preprocessing.LabelEncoder): Label encoder\n",
    "        \"\"\"\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "        from sklearn.metrics import classification_report\n",
    "        print(\"Classification Report:\")\n",
    "        print(classification_report(y_test, y_pred_classes, target_names=label_encoder.classes_))\n",
    "    \n",
    "    def run_pipeline(self):\n",
    "        \"\"\"\n",
    "        Run the complete voice emotion detection pipeline.\n",
    "        \"\"\"\n",
    "        audio_df = self.create_metadata_dataframe()\n",
    "        \n",
    "        print(\"Dataset Summary:\")\n",
    "        print(audio_df['emotion'].value_counts())\n",
    "\n",
    "        X, y, label_encoder = self.prepare_dataset(audio_df)\n",
    "\n",
    "        model = self.train_and_evaluate(X, y, label_encoder)\n",
    "\n",
    "        model.save('final_voice_emotion_model.h5')\n",
    "        joblib.dump(label_encoder, 'voice_emotion_label_encoder.pkl')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    dataset_path = r'C:\\Users\\aksha\\Downloads\\RAVDESS\\audio_speech_actors_01-24'\n",
    "    detector = AdvancedVoiceEmotionDetector(dataset_path)\n",
    "    detector.run_pipeline()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad8967d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1185394",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
